# üéØ Solution d'Observabilit√© Compl√®te - R√©sum√© Ex√©cutif

## ‚úÖ Ce Qui a √ât√© Cr√©√©

### Stack d'Observabilit√© Production-Ready

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    APPLICATION                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ  Frontend   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄHTTP‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ   Backend    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  (React +   ‚îÇ              ‚îÇ   (Express + ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ   Nginx)    ‚îÇ              ‚îÇ  OpenTelemetry)             ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                        ‚îÇ
                    OTLP (gRPC/HTTP 4317/4318)
                                        ‚îÇ
                                        ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ OpenTelemetry Collector    ‚îÇ
                    ‚îÇ ‚Ä¢ Receive (OTLP)           ‚îÇ
                    ‚îÇ ‚Ä¢ Process (Batch, Filter)  ‚îÇ
                    ‚îÇ ‚Ä¢ Export (Multi-backend)   ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ      ‚îÇ      ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚ñº                   ‚ñº                    ‚ñº
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  TEMPO  ‚îÇ         ‚îÇ  LOKI   ‚îÇ         ‚îÇPROMETHEUS ‚îÇ
        ‚îÇ         ‚îÇ         ‚îÇ    +    ‚îÇ         ‚îÇ     +     ‚îÇ
        ‚îÇ Traces  ‚îÇ         ‚îÇPROMTAIL ‚îÇ         ‚îÇ OPERATOR  ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                   ‚îÇ                     ‚îÇ
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚ñº
                           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                           ‚îÇ GRAFANA  ‚îÇ
                           ‚îÇ          ‚îÇ
                           ‚îÇ Unified  ‚îÇ
                           ‚îÇ   View   ‚îÇ
                           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Fichiers Cr√©√©s

### Backend API (Node.js/Express)

```
backend/
‚îú‚îÄ‚îÄ server.js              # API avec endpoints /api/todos
‚îú‚îÄ‚îÄ tracing.js             # OpenTelemetry SDK configuration
‚îú‚îÄ‚îÄ logger.js              # Pino structured logging
‚îú‚îÄ‚îÄ metrics.js             # Prometheus metrics (prom-client)
‚îú‚îÄ‚îÄ package.json           # Dependencies
‚îî‚îÄ‚îÄ Dockerfile             # Container image
```

**Instrumentation** :
- ‚úÖ OpenTelemetry auto-instrumentation (HTTP, Express)
- ‚úÖ Spans manuels pour business logic
- ‚úÖ Structured JSON logs avec traceId
- ‚úÖ Prometheus metrics (RED + business)
- ‚úÖ Export vers OpenTelemetry Collector

### Kubernetes Manifests

```
k8s/
‚îú‚îÄ‚îÄ namespace.yaml                    # tp2devops + observability
‚îú‚îÄ‚îÄ otel-collector.yaml               # OpenTelemetry Collector
‚îú‚îÄ‚îÄ prometheus-operator.yaml          # Prometheus + ServiceMonitor
‚îú‚îÄ‚îÄ loki-stack.yaml                   # Loki + Promtail DaemonSet
‚îú‚îÄ‚îÄ tempo.yaml                        # Tempo tracing backend
‚îú‚îÄ‚îÄ grafana.yaml                      # Grafana + datasources
‚îú‚îÄ‚îÄ application.yaml                  # Frontend + Backend deployment
‚îî‚îÄ‚îÄ grafana-dashboards/
    ‚îî‚îÄ‚îÄ application-dashboard.json    # Pre-configured dashboard
```

### Docker & Compose

```
docker-compose.yml                    # Stack compl√®te pour dev local
Dockerfile                            # Frontend (React + Nginx)
Dockerfile.dev                        # Frontend dev mode
backend/Dockerfile                    # Backend production image
```

### Configuration Files

```
config/
‚îú‚îÄ‚îÄ tempo.yaml                        # Tempo configuration
‚îú‚îÄ‚îÄ prometheus.yml                    # Prometheus scrape config
‚îú‚îÄ‚îÄ grafana-datasources.yml           # Auto-provisioned datasources
‚îî‚îÄ‚îÄ otel-collector-config.yaml        # Collector pipelines
```

### Deployment Scripts

```
scripts/
‚îú‚îÄ‚îÄ setup-observability.sh            # Deploy observability stack
‚îú‚îÄ‚îÄ deploy-application.sh             # Build & deploy app
‚îî‚îÄ‚îÄ cleanup.sh                        # Clean everything
```

### Documentation

```
OBSERVABILITY.md                      # Documentation technique compl√®te
README_OBSERVABILITY.md               # Vue d'ensemble
DEPLOYMENT_GUIDE.md                   # Guide de d√©ploiement
```

### CI/CD

```
.github/workflows/
‚îú‚îÄ‚îÄ ci.yml                            # Pipeline original (GitHub Pages)
‚îî‚îÄ‚îÄ ci-k8s.yml                        # Pipeline Docker + K8s
```

---

## üéØ Les 3 Piliers Impl√©ment√©s

### 1. TRACES (Tempo + OpenTelemetry)

**Impl√©mentation** :
```javascript
// backend/tracing.js
import { NodeSDK } from '@opentelemetry/sdk-node';
import { OTLPTraceExporter } from '@opentelemetry/exporter-trace-otlp-http';

const sdk = new NodeSDK({
  traceExporter: new OTLPTraceExporter({
    url: `${OTEL_COLLECTOR_URL}/v1/traces`,
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});
```

**R√©sultat** :
- ‚úÖ Traces distribu√©es de bout en bout
- ‚úÖ Flame graphs dans Grafana
- ‚úÖ Correlation traces ‚Üí logs ‚Üí metrics
- ‚úÖ Export vers Tempo via OTLP

**Exemple de trace** :
```
POST /api/todos [15ms]
‚îú‚îÄ create_todo [12ms]
‚îÇ  ‚îú‚îÄ validation [1ms]
‚îÇ  ‚îú‚îÄ todo_creation [8ms]
‚îÇ  ‚îî‚îÄ metrics_update [2ms]
‚îî‚îÄ response_send [1ms]
```

### 2. METRICS (Prometheus + Operator)

**Impl√©mentation** :
```javascript
// backend/metrics.js
import promClient from 'prom-client';

const httpRequestsTotal = new promClient.Counter({
  name: 'tp2devops_http_requests_total',
  help: 'Total HTTP requests',
  labelNames: ['method', 'route', 'status_code'],
});

const httpRequestDuration = new promClient.Histogram({
  name: 'tp2devops_http_request_duration_seconds',
  help: 'HTTP request duration',
  buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5],
});
```

**M√©triques disponibles** :

| M√©trique | Type | Description |
|----------|------|-------------|
| `tp2devops_http_requests_total` | Counter | Requ√™tes HTTP par method/route/status |
| `tp2devops_http_request_duration_seconds` | Histogram | Latence des requ√™tes (buckets) |
| `tp2devops_todo_operations_total` | Counter | Op√©rations CRUD sur todos |
| `tp2devops_todos_active` | Gauge | Nombre de todos actifs |
| `tp2devops_todos_completed` | Gauge | Nombre de todos compl√©t√©s |

**Prometheus Operator** :
- ‚úÖ CRDs : `Prometheus`, `ServiceMonitor`, `PrometheusRule`
- ‚úÖ Auto-discovery des endpoints via labels
- ‚úÖ Remote Write pour OpenTelemetry Collector

### 3. LOGS (Loki + Promtail)

**Impl√©mentation** :
```javascript
// backend/logger.js
import pino from 'pino';

const logger = pino({
  level: 'info',
  formatters: {
    level: (label) => ({ level: label.toUpperCase() }),
  },
  timestamp: pino.stdTimeFunctions.isoTime,
});

// Avec correlation
logger.info({ msg: 'Todo created', todoId: 123, traceId: '...' });
```

**Format de log** :
```json
{
  "level": "INFO",
  "time": "2025-10-19T16:30:00.000Z",
  "msg": "Todo created",
  "todoId": 123,
  "text": "Example",
  "traceId": "a1b2c3d4e5f6..."
}
```

**Collection** :
- ‚úÖ Promtail DaemonSet sur chaque n≈ìud
- ‚úÖ Collecte automatique depuis `/var/log/pods`
- ‚úÖ Labels Kubernetes auto-ajout√©s
- ‚úÖ Export vers Loki

**Requ√™tes LogQL** :
```logql
{namespace="tp2devops", app="backend"}              # Tous les logs backend
{namespace="tp2devops"} |= "ERROR"                  # Logs d'erreur
{namespace="tp2devops"} | json | traceId="abc123"  # Logs d'une trace
```

---

## üîó Corr√©lation des Donn√©es

### Traces ‚Üí Logs

**Dans le code** :
```javascript
const traceId = trace.getSpan(context.active())?.spanContext().traceId;
logger.info({ msg: "...", traceId: traceId });
```

**Dans Grafana** :
1. Explore ‚Üí Tempo ‚Üí S√©lectionner une trace
2. Clic sur "Logs for this span"
3. ‚Üí Automatiquement filtr√© par `traceId` dans Loki

### Logs ‚Üí Traces

**Dans Grafana** :
1. Explore ‚Üí Loki ‚Üí Voir les logs
2. Clic sur un `traceId` dans les logs
3. ‚Üí Ouvre la trace correspondante dans Tempo

### Metrics ‚Üí Traces (Exemplars)

**Configuration Prometheus** :
```yaml
# Permet de lier m√©triques et traces
enable-feature=exemplar-storage
```

---

## üöÄ D√©ploiement

### Docker Compose (Local)

```bash
docker-compose up -d

# Acc√®s
Frontend:   http://localhost:5173
Backend:    http://localhost:3001
Grafana:    http://localhost:3000  (admin/admin)
Prometheus: http://localhost:9090
Loki:       http://localhost:3100
Tempo:      http://localhost:3200
```

### Kubernetes (Minikube)

```bash
# 1. D√©marrer Minikube
minikube start --cpus=4 --memory=8192

# 2. D√©ployer observabilit√©
./scripts/setup-observability.sh

# 3. D√©ployer application
./scripts/deploy-application.sh

# Acc√®s
Frontend: http://$(minikube ip):30080
Grafana:  http://$(minikube ip):30300 (admin/admin)
```

### Ordre de D√©ploiement

1. ‚úÖ Namespaces
2. ‚úÖ Prometheus Operator (CRDs)
3. ‚úÖ Prometheus instance
4. ‚úÖ Loki + Promtail
5. ‚úÖ Tempo
6. ‚úÖ OpenTelemetry Collector
7. ‚úÖ Grafana
8. ‚úÖ Application (Backend + Frontend)

---

## üìä Visualisation dans Grafana

### Datasources Auto-Configur√©es

```yaml
datasources:
  - Prometheus (default)
    url: http://prometheus-operated:9090
    
  - Loki
    url: http://loki:3100
    tracesToLogsV2: enabled  # Correlation
    
  - Tempo
    url: http://tempo:3200
    tracesToLogs: enabled
    tracesToMetrics: enabled
```

### Dashboard Application

- HTTP Request Rate (Graph)
- HTTP Request Duration P50/P95/P99 (Graph)
- Active Todos (Stat)
- Completed Todos (Stat)
- Todo Operations Rate (Graph)
- Error Rate (Graph)
- Recent Logs (Logs panel with traceId)

### Features Enabled

- ‚úÖ TraceQL Editor
- ‚úÖ Traces Embedded Flame Graph
- ‚úÖ Logs correlation
- ‚úÖ Metrics correlation
- ‚úÖ Service Graph

---

## üéì Points Cl√©s d'Architecture

### Pourquoi OpenTelemetry Collector ?

**Avantages** :
- ‚úÖ **D√©couplage** : App ‚Üí Collector ‚Üí N backends
- ‚úÖ **Flexibilit√©** : Changer de backend sans modifier l'app
- ‚úÖ **Reliability** : Buffering, retry, backpressure
- ‚úÖ **Processing** : Sampling, filtering, enrichment centralis√©
- ‚úÖ **Multi-tenancy** : Routage conditionnel

### Pourquoi Prometheus Operator ?

**Avantages** :
- ‚úÖ **D√©claratif** : Configuration via CRDs (GitOps friendly)
- ‚úÖ **Auto-discovery** : ServiceMonitor d√©tecte les endpoints
- ‚úÖ **Scalable** : Multi-prometheus, sharding
- ‚úÖ **Alerting** : PrometheusRule pour les alertes

### Pourquoi cette Architecture ?

**Production-Ready** :
- ‚úÖ Haute disponibilit√© (tous les composants peuvent scaler)
- ‚úÖ S√©paration des pr√©occupations (3 backends sp√©cialis√©s)
- ‚úÖ Observabilit√© de l'observabilit√© (metrics du collector)
- ‚úÖ Standards ouverts (OpenTelemetry, PromQL, LogQL, TraceQL)

---

## üìà M√©triques et Requ√™tes Utiles

### Prometheus Queries

```promql
# Taux de requ√™tes HTTP
rate(tp2devops_http_requests_total[5m])

# Latence P95
histogram_quantile(0.95, 
  rate(tp2devops_http_request_duration_seconds_bucket[5m])
)

# Taux d'erreurs (5xx)
rate(tp2devops_http_requests_total{status_code=~"5.."}[5m])

# Todos actifs
tp2devops_todos_active

# Taux d'op√©rations par type
rate(tp2devops_todo_operations_total[5m])
```

### Loki Queries (LogQL)

```logql
# Tous les logs backend
{namespace="tp2devops", app="backend"}

# Logs d'erreur
{namespace="tp2devops"} |= "ERROR"

# Logs avec trace ID
{namespace="tp2devops"} | json | traceId="abc123"

# Agr√©gation: taux d'erreurs
rate({namespace="tp2devops"} |= "ERROR" [5m])

# Filtrage JSON
{namespace="tp2devops"} | json | level="ERROR" | todoId > 100
```

### Tempo Queries (TraceQL)

```traceql
# Toutes les traces du service
{ resource.service.name = "tp2devops-backend" }

# Traces avec latence > 100ms
{ duration > 100ms }

# Traces d'une op√©ration sp√©cifique
{ name = "create_todo" }

# Traces avec erreur
{ status = error }
```

---

## üîç Debugging Workflow

### Sc√©nario : Latence √âlev√©e D√©tect√©e

1. **Grafana Dashboard** : P95 latency > 500ms
2. **Prometheus** : Identifier l'endpoint lent
   ```promql
   histogram_quantile(0.95, 
     rate(tp2devops_http_request_duration_seconds_bucket{route="/api/todos"}[5m])
   )
   ```
3. **Loki** : Chercher les requ√™tes lentes
   ```logql
   {namespace="tp2devops", app="backend"} 
     | json 
     | duration > 500
   ```
4. **Tempo** : Analyser une trace sp√©cifique
   - Copier le `traceId` du log
   - Rechercher dans Tempo
   - Voir la flame graph
5. **Root Cause** : Identifier le span lent
6. **Fix** : Optimiser le code identifi√©

---

## üìö Documentation Cr√©√©e

| Fichier | Description |
|---------|-------------|
| `OBSERVABILITY.md` | Documentation technique compl√®te |
| `README_OBSERVABILITY.md` | Vue d'ensemble et quick start |
| `DEPLOYMENT_GUIDE.md` | Guide de d√©ploiement d√©taill√© |
| `COMPLETE_OBSERVABILITY_SUMMARY.md` | Ce fichier - r√©sum√© ex√©cutif |

---

## ‚úÖ Checklist de Validation

### Observabilit√© Op√©rationnelle

- [x] **Traces** : Visibles dans Tempo, flame graphs fonctionnels
- [x] **Metrics** : Prometheus scrape le backend, dashboards affichent data
- [x] **Logs** : Loki re√ßoit les logs, requ√™tes LogQL fonctionnent
- [x] **Correlation** : Traces ‚Üí Logs ‚Üí Metrics navigation fonctionne
- [x] **Grafana** : 3 datasources configur√©es et op√©rationnelles

### Infrastructure

- [x] **Kubernetes** : Tous les pods Running
- [x] **PVC** : Storage provisionn√© pour Prometheus, Loki, Tempo, Grafana
- [x] **ServiceMonitor** : Prometheus d√©couvre les targets automatiquement
- [x] **Promtail** : DaemonSet collecte logs de tous les n≈ìuds
- [x] **OpenTelemetry Collector** : Re√ßoit et exporte correctement

### Application

- [x] **Backend** : Health check OK, metrics endpoint r√©pond
- [x] **Frontend** : Application accessible, fonctionnelle
- [x] **API** : CRUD operations fonctionnent
- [x] **Instrumentation** : Tous les endpoints trac√©s

---

## üéØ R√©sultat Final

### Ce Que Nous Avons

‚úÖ **Stack d'observabilit√© compl√®te** :
- Prometheus + Operator pour les m√©triques
- Loki + Promtail pour les logs
- Tempo pour les traces
- OpenTelemetry Collector comme point central
- Grafana pour la visualisation unifi√©e

‚úÖ **Application instrument√©e** :
- Backend Express avec OpenTelemetry SDK
- Logs structur√©s JSON avec traceId
- M√©triques Prometheus (RED + business)
- Auto-instrumentation + spans manuels

‚úÖ **D√©ploiement automatis√©** :
- Scripts de d√©ploiement K8s
- Docker Compose pour dev local
- CI/CD pipeline avec Docker builds

‚úÖ **Documentation compl√®te** :
- Architecture d√©taill√©e
- Guides de d√©ploiement
- Requ√™tes d'exemple
- Workflows de debugging

---

## üöÄ Prochaines √âtapes Possibles

### Am√©liorations

1. **Alerting** : Configurer Alertmanager avec PrometheusRule
2. **Service Mesh** : Ajouter Istio pour tracing automatique
3. **Dashboards** : Cr√©er plus de dashboards (SLO, SLI)
4. **Sampling** : Configurer tail-based sampling dans Collector
5. **Exemp

lars** : Lier m√©triques et traces
6. **Multi-cluster** : F√©d√©ration Prometheus
7. **Long-term storage** : S3/GCS pour Loki et Tempo

### Production Hardening

1. **HA** : R√©plicas multiples pour tous les composants
2. **Security** : TLS, authentication, RBAC
3. **Backup** : PVC snapshots, retention policies
4. **Monitoring** : Alertes sur les composants d'observabilit√©
5. **Resource limits** : Tuning CPU/Memory
6. **Ingress** : Load balancing, SSL termination

---

**üéâ Solution d'observabilit√© production-ready avec les 3 piliers complets ! üéâ**

Stack : **Prometheus ‚Ä¢ Loki ‚Ä¢ Tempo ‚Ä¢ OpenTelemetry ‚Ä¢ Grafana**

